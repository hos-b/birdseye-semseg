[ ] add possibility for bigger batch size
[ ] perturbations to pitch
[ ] increase network capacity
[ ] add resuming possibility

retry now that shit works:
[ ] different kinds of latent masking
[ ] weighted losses

incorrect fast scnn implementation:
* pyramid pooling uses the base channel size in mid layers
	- should be (base channel size // number of stages)

* missing normalization in the bottleneck at the end of pyramid pooling
* missing dropout in the classifier
